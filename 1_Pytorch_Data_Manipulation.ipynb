{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIDcVmM+tZxsgN13n3a92P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/demirbey05/jarvis.dl/blob/main/1_Pytorch_Data_Manipulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLudAaxZnhza"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(12,dtype=torch.float16)\n",
        "print(x.shape)\n",
        "print(x.reshape(12,1).shape)\n",
        "print(x.numel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RRxmud8nmNQ",
        "outputId": "447feb21-ce44-4242-8893-1c2f3e7e11f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([12])\n",
            "torch.Size([12, 1])\n",
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indexing and Slicing\n",
        "\n",
        "Indexed or sliced value shares same data with original tensor, so if you modify it will reflect to original one"
      ],
      "metadata": {
        "id": "10JrZDfzoZF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = x.reshape(3,4)\n",
        "print(X)\n",
        "print(X[:,1]) #second column\n",
        "print(X[-1,:]) #last row\n",
        "print(X[1]) # second row"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAIe5_mjn3iW",
        "outputId": "f7d6cadb-8247-415f-d5ea-5d3f4597f257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.]], dtype=torch.float16)\n",
            "tensor([1., 5., 9.], dtype=torch.float16)\n",
            "tensor([ 8.,  9., 10., 11.], dtype=torch.float16)\n",
            "tensor([4., 5., 6., 7.], dtype=torch.float16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T = X[:,2]\n",
        "print(T)\n",
        "print(T.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xt0JWayVotaS",
        "outputId": "67cb1a7d-8565-4ef6-aca4-b8928acbc630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.,  6., 10.], dtype=torch.float16)\n",
            "torch.Size([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T[0] = 1000.0\n",
        "print(X)\n",
        "# You will see X[0,2] will be 1000\n",
        "print(X[0,2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R71qbn2ppKZc",
        "outputId": "ed5dbb71-c4a1-4dda-a2a7-9afdada2af79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   0.,    1., 1000.,    3.],\n",
            "        [   4.,    5.,    6.,    7.],\n",
            "        [   8.,    9.,   10.,   11.]], dtype=torch.float16)\n",
            "tensor(1000., dtype=torch.float16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WARN:** `X[start:stop]`, where\n",
        "**the returned value includes the first index (start) but not the last (stop).**"
      ],
      "metadata": {
        "id": "b_Z3hma5pxFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Broadcasting"
      ],
      "metadata": {
        "id": "ADpq5KJLtsD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.arange(3).reshape((3, 1))\n",
        "b = torch.arange(2).reshape((1, 2))\n",
        "\n",
        "a,b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYIE4aYgp9l9",
        "outputId": "aac879f0-7826-43bf-ad7c-aea015713a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0],\n",
              "         [1],\n",
              "         [2]]),\n",
              " tensor([[0, 1]]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a + b  # must be (3,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hgjmh0--tu0N",
        "outputId": "c56f381b-1fe7-4598-cba6-76c53f0c88db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [1, 2],\n",
              "        [2, 3]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two tensors are “broadcastable” if the following rules hold:\n",
        "\n",
        "- Each tensor has at least one dimension.\n",
        "- When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must **either be equal, one of them is 1, or one of them does not exist.**"
      ],
      "metadata": {
        "id": "pjblulaivoyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best way to save memory is to use in-place operations, let's show with `id()` function which shows memory address of the object"
      ],
      "metadata": {
        "id": "f3I83m9jv0Qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.arange(12,dtype=torch.bfloat16).reshape(3,4)\n",
        "Y = torch.ones_like(X)\n",
        "print(X,Y)\n",
        "print(f'Memory address of X: {id(X)}')\n",
        "print(f'Memory address of Y: {id(Y)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZ0nGv5ht1ei",
        "outputId": "43f4e52a-f512-4f4e-f82d-c556f691ca5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.]], dtype=torch.bfloat16) tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]], dtype=torch.bfloat16)\n",
            "Memory address of X: 132781567047536\n",
            "Memory address of Y: 132781567040624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Z = X + Y\n",
        "print(id(Z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xjk1om1VwGKG",
        "outputId": "44de9d91-a479-49b7-9623-795c9cbfb9ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "132781567048112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can directly change the data in X\n",
        "X[:] = X + Y\n",
        "# Or X += Y"
      ],
      "metadata": {
        "id": "fju8DDtTwZiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Algebra"
      ],
      "metadata": {
        "id": "0XbO6OHrxhbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show that transpose of transpose equal to original matrix\n",
        "A= torch.arange(12,dtype=torch.float32).reshape(3,4)\n",
        "B = A.T\n",
        "torch.equal(A,B.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzD1IYF4xBQM",
        "outputId": "aa1bc000-fa25-4e34-da56-f5a322e5be95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We defined the tensor X of shape (2, 3, 4) in this section. What is the output of len(X)?\n",
        "X = torch.ones((2,3,4))\n",
        "len(X) # 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_thmbQQRxwv0",
        "outputId": "6389da9f-7dad-405a-d06c-3b6e42fcb43c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "K = torch.arange(12,dtype=torch.float32).reshape(3,4)\n",
        "\n",
        "print(id(torch.transpose(K,1,0)))\n",
        "print(id(K))"
      ],
      "metadata": {
        "id": "sCFjwgAbyG7E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98ff4806-3894-4c62-d3f4-8bf92c03b5c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "132781567043024\n",
            "132781567047248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How Pytorch Stores Data"
      ],
      "metadata": {
        "id": "pqRPN7ZpYZyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensor contains 5 fundamental attributes : `size`,`stride`,`device`,`type`,`layout`. **Pytorch stores tensor with strided way in a memory.**\n",
        "\n"
      ],
      "metadata": {
        "id": "1Bjt8J4ueIPM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "acNFG-_kYcPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autograd"
      ],
      "metadata": {
        "id": "q5_2xH7Uj-Qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take derivative of x.T @ x respect to x (Scalar Valued Function !!!!)\n",
        "\n",
        "x = torch.arange(3,dtype=torch.bfloat16,requires_grad =True) # requires_grad must be present\n",
        "print(x.grad) # None by default because we didnt do any process"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYxbfh8bj_l2",
        "outputId": "c19ab227-26ee-488b-c45d-39d40af737f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x.T @ x\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sdb8T1RLk5qw",
        "outputId": "e110c629-8162-4a27-ff45-64b3533b666d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-327d0d64c3d4>:1: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3725.)\n",
            "  y = x.T @ x\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5., dtype=torch.bfloat16, grad_fn=<DotBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pay Attention : `grad_fn=<DotBackward0>`"
      ],
      "metadata": {
        "id": "xMdQmtvelTj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To fill x's grad vector\n",
        "y.backward()\n",
        "print(x.grad)\n",
        "print(torch.equal(x.grad,2 * x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQoGjCFClNVz",
        "outputId": "d1a833b0-ab62-4438-9e64-41e600e65880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 2., 4.], dtype=torch.bfloat16)\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For another operation we need to reset gradient, because torch is not supporting\n",
        "x.grad.zero_()\n",
        "x.sum().backward()\n",
        "torch.equal(x.grad,torch.ones_like(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOD_VZk1lkag",
        "outputId": "18cbf3ab-a734-4133-de4e-081dd5f13c69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Now for Non-Scalar Valued Functions\n",
        "## For non-scalar we have to transform non-scalar to scalar\n",
        "\n",
        "x.grad.zero_()\n",
        "y = x * x\n",
        "y.sum().backward()\n"
      ],
      "metadata": {
        "id": "WrJx56GNpZYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "id": "ifqmJR71IpOR",
        "outputId": "e25018e2-9104-41c2-89ff-1e85afa02291",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4.], dtype=torch.bfloat16)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SPS2S94qIuz0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}